{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e482c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f2672fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names_es_ar.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4a792b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(\"\".join(words))))\n",
    "n = len(chars)\n",
    "\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "\n",
    "itos = {i: s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "891ddcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 1461217\n"
     ]
    }
   ],
   "source": [
    "N = torch.zeros((n + 1, n + 1), dtype=torch.int32)\n",
    "compile = 0\n",
    "xs, ys = [], []\n",
    "for w in words[:]:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print(\"number of examples:\", num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca7dced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  4, 33,  ..., 40, 40, 29])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1b02398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 33, 42,  ..., 40, 29,  0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "975da2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize neural network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((n + 1, n + 1), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b7214c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.734150409698486\n",
      "loss: 4.231932163238525\n",
      "loss: 3.9133219718933105\n",
      "loss: 3.676642656326294\n",
      "loss: 3.509608268737793\n",
      "loss: 3.3883750438690186\n",
      "loss: 3.2922098636627197\n",
      "loss: 3.212552547454834\n",
      "loss: 3.1451900005340576\n",
      "loss: 3.0874204635620117\n",
      "loss: 3.03688645362854\n",
      "loss: 2.9925291538238525\n",
      "loss: 2.9530739784240723\n",
      "loss: 2.9179892539978027\n",
      "loss: 2.8862547874450684\n",
      "loss: 2.8577513694763184\n",
      "loss: 2.831519842147827\n",
      "loss: 2.807795286178589\n",
      "loss: 2.785609245300293\n",
      "loss: 2.7654640674591064\n",
      "loss: 2.746267318725586\n",
      "loss: 2.7289130687713623\n",
      "loss: 2.7121357917785645\n",
      "loss: 2.6969995498657227\n",
      "loss: 2.6820852756500244\n",
      "loss: 2.6687750816345215\n",
      "loss: 2.6554625034332275\n",
      "loss: 2.643681049346924\n",
      "loss: 2.631643533706665\n",
      "loss: 2.621112585067749\n",
      "loss: 2.6101486682891846\n",
      "loss: 2.6007158756256104\n",
      "loss: 2.590719223022461\n",
      "loss: 2.5821995735168457\n",
      "loss: 2.5730316638946533\n",
      "loss: 2.565351724624634\n",
      "loss: 2.556913375854492\n",
      "loss: 2.5499250888824463\n",
      "loss: 2.542113780975342\n",
      "loss: 2.5357840061187744\n",
      "loss: 2.528477191925049\n",
      "loss: 2.5227267742156982\n",
      "loss: 2.515958070755005\n",
      "loss: 2.5106160640716553\n",
      "loss: 2.5042600631713867\n",
      "loss: 2.499357223510742\n",
      "loss: 2.4934237003326416\n",
      "loss: 2.4889075756073\n",
      "loss: 2.483332633972168\n",
      "loss: 2.4791746139526367\n",
      "loss: 2.473945379257202\n",
      "loss: 2.4701263904571533\n",
      "loss: 2.465146780014038\n",
      "loss: 2.4616074562072754\n",
      "loss: 2.456941843032837\n",
      "loss: 2.4536666870117188\n",
      "loss: 2.4492506980895996\n",
      "loss: 2.4462480545043945\n",
      "loss: 2.442004680633545\n",
      "loss: 2.439208507537842\n",
      "loss: 2.435192108154297\n",
      "loss: 2.4326038360595703\n",
      "loss: 2.428727865219116\n",
      "loss: 2.426299810409546\n",
      "loss: 2.422605276107788\n",
      "loss: 2.4203524589538574\n",
      "loss: 2.416839599609375\n",
      "loss: 2.414705276489258\n",
      "loss: 2.411320686340332\n",
      "loss: 2.4093587398529053\n",
      "loss: 2.4061615467071533\n",
      "loss: 2.404346466064453\n",
      "loss: 2.4012322425842285\n",
      "loss: 2.3995351791381836\n",
      "loss: 2.3965539932250977\n",
      "loss: 2.394982099533081\n",
      "loss: 2.3920841217041016\n",
      "loss: 2.3905997276306152\n",
      "loss: 2.387848377227783\n",
      "loss: 2.38647723197937\n",
      "loss: 2.3837974071502686\n",
      "loss: 2.382530927658081\n",
      "loss: 2.3799424171447754\n",
      "loss: 2.3787682056427\n",
      "loss: 2.3762662410736084\n",
      "loss: 2.3752224445343018\n",
      "loss: 2.3727965354919434\n",
      "loss: 2.3718361854553223\n",
      "loss: 2.3694746494293213\n",
      "loss: 2.3685824871063232\n",
      "loss: 2.366328716278076\n",
      "loss: 2.3654916286468506\n",
      "loss: 2.3633267879486084\n",
      "loss: 2.3625409603118896\n",
      "loss: 2.3603837490081787\n",
      "loss: 2.3596739768981934\n",
      "loss: 2.357584238052368\n",
      "loss: 2.356940984725952\n",
      "loss: 2.3549036979675293\n",
      "loss: 2.3543100357055664\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 50\n",
    "L2_REGULARIZATION_STRENGTH = 0.01\n",
    "\n",
    "for k in range(100):\n",
    "\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=n + 1).float()\n",
    "    logists = xenc @ W  # predict log-counts\n",
    "    counts = logists.exp()  # counts, equivalent to N\n",
    "    probs = counts / counts.sum(dim=1, keepdim=True)  # probabilities for next character\n",
    "    loss = (\n",
    "        -probs[torch.arange(num), ys].log().mean()\n",
    "        + L2_REGULARIZATION_STRENGTH * (W**2).mean()\n",
    "    )\n",
    "    print(f\"loss: {loss.item()}\")\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None  # set to zero the gradient\n",
    "    loss.backward()  # compute the gradient of the loss with respect to W\n",
    "\n",
    "    # update the weights\n",
    "    W.data += -LEARNING_RATE * W.grad  # update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca72a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa47158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ana\n",
      "Gia\n",
      "Satomiziara\n",
      "Adéilima\n",
      "NÓFEZñEla\n",
      "Dan\n",
      "OfHJuchefJa\n",
      "Yjatelea\n",
      "Arily\n",
      "CJaexCa\n"
     ]
    }
   ],
   "source": [
    "# Generate 10 samples, omitting the last character ('.') in the output\n",
    "num_samples = 10\n",
    "for _ in range(num_samples):\n",
    "    ix = 0\n",
    "    generated_indices = []\n",
    "    while True:\n",
    "        # One-hot encode the current index\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=n + 1).float()\n",
    "        # Compute logits and probabilities\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "        # Sample the next character index from the probability distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        generated_indices.append(ix)\n",
    "        # If the sampled character is '.', break\n",
    "        if itos[ix] == '.':\n",
    "            break\n",
    "    # Convert indices to characters, omit the last character ('.'), and join to form the generated string\n",
    "    generated_string = ''.join([itos[i] for i in generated_indices[:-1]])\n",
    "    print(generated_string)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
